{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstęp \n",
    "Preprocessing danych jest pierwszym i jednocześnie jednym z najważniejszych etapów, każdego projektu związanego z danologią (data science) oraz uczeniem maszynowym. Preprocessing skupia się na przetworzeniu posiadanych danych do postaci odpowiadającej wymaganiom danego zadania. Najczęściej polega to na usunięciu błędów takich jak brakujących lub niepotrzebnych danych (szumów). W tym dokumencie, chciałbym skupić się na procesie preprocessingu stosowanym w gałęzi uczenia maszynowego o nazwie **NLP** (ang. **Natural Language Processing**) czyli Przetwarzanie Języka Naturalnego, która zajmuje się badaniem możliwości AI w zadaniach dotyczących zagadnień językowych, takich jak rozumienie tekstu, tworzenie podsumowań, inteligentnych botów itp.\n",
    "<br>\n",
    "Do najczęściej wykorzystywanych kroków preprocessingu, należy zaliczyć:\n",
    "- Usuwanie hiperlinków\n",
    "- Usuwanie tagów HTML\n",
    "- Stemming\n",
    "- Lematyzacja\n",
    "- Standaryzacja wielkości liter (tj. A→a)\n",
    "- Usuwanie stopwordów\n",
    "- Usuwanie częstych/rzadkich słów\n",
    "- Usuwanie znaków interpunkcyjnych\n",
    "- Usuwanie emotikon/emoji (szczególnie w przypadku danych z mediów społecznościowych)\n",
    "- Przetwarzanie emotikon do słów\n",
    "- Poprawianie błędów językowych\n",
    "\n",
    "<br>\n",
    "Każdy problem NLP wymaga własnego specyficznego podejścia przez co wykorzystywane procesy przetwarzania danych mogą się różnić w zależności od celów projektu.\n",
    "<br>\n",
    "Poniższy tekst w dużej mierze bazuje na gotowym kernelu, który można znaleźć pod linkiem:\n",
    "<br>\n",
    "https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wymagane pakiety\n",
    "Lista wymaganych do instalacji pakietów to:\n",
    "- numpy | podstawowe operacje na obiektach\n",
    "- pandas | przetwarzanie danych z pliku\n",
    "- nltk | stemming/lematyzacja\n",
    "- bs4 | operacje na plikach .html (usuwanie tagów)\n",
    "- lxml | wymagane do działania BeautifulSoup\n",
    "- pyspellchecker | biblioteka do sprawdzania poprawności zapisu\n",
    "- morfeusz2 | biblioteka do analizy morfologicznej języka polskiego\n",
    "- string\n",
    "- collections\n",
    "- os\n",
    "- requests\n",
    "\n",
    "<br>\n",
    "W przypadku uruchomienia poniższego kodu, nastąpi pobranie (prawie) wszystkich wymaganych pakietów oraz plików. Może być wymagane ponowne uruchomienia Kernela w celu załadowania niektórych bibliotek.\n",
    "<br>\n",
    "Jedynym plikiem, którego pobieranie nie jest zautomatyzowane jest plik z danymi.\n",
    "<br>\n",
    "Nalezy pobrac plik .csv spod linku https://www.kaggle.com/kazanova/sentiment140\n",
    "<br>\n",
    "Domyślnie ścieżka do ładowanego pliku wymaga umieszczenia go w folderze w którym znajduje się notatnik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install numpy\n",
    "%conda install pandas\n",
    "%conda install nltk\n",
    "%conda install bs4\n",
    "%conda install lxml\n",
    "%conda install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing morfeusz2-1.9.16-py3.7-win-amd64.egg\n",
      "removing 'c:\\users\\senuk\\anaconda3\\envs\\preprocessing\\lib\\site-packages\\morfeusz2-1.9.16-py3.7-win-amd64.egg' (and everything under it)\n",
      "creating c:\\users\\senuk\\anaconda3\\envs\\preprocessing\\lib\\site-packages\\morfeusz2-1.9.16-py3.7-win-amd64.egg\n",
      "Extracting morfeusz2-1.9.16-py3.7-win-amd64.egg to c:\\users\\senuk\\anaconda3\\envs\\preprocessing\\lib\\site-packages\n",
      "morfeusz2 1.9.16 is already the active version in easy-install.pth\n",
      "\n",
      "Installed c:\\users\\senuk\\anaconda3\\envs\\preprocessing\\lib\\site-packages\\morfeusz2-1.9.16-py3.7-win-amd64.egg\n",
      "Processing dependencies for morfeusz2==1.9.16\n",
      "Finished processing dependencies for morfeusz2==1.9.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The easy_install command is deprecated and will be removed in a future version.\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "'''\n",
    "download\n",
    "Funckja download jest wykorzystywana w celu pobierania wymaganych plików\n",
    "źródło: https://stackoverflow.com/questions/22676/how-do-i-download-a-file-over-http-using-python\n",
    "\n",
    "Parametry:\n",
    "    url: adres html pod którym znajduje się porządany plik\n",
    "'''\n",
    "def download(url: str):\n",
    "    get_response = requests.get(url,stream=True)\n",
    "    file_name  = url.split(\"/\")[-1]\n",
    "    with open(file_name, 'wb') as f:\n",
    "        for chunk in get_response.iter_content(chunk_size=1024):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "download(\"http://download.sgjp.pl/morfeusz/20200906/Windows/64/morfeusz2-1.9.16-py3.7-win-amd64.egg\")\n",
    "!easy_install morfeusz2-1.9.16-py3.7-win-amd64.egg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import morfeusz2\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opis danych\n",
    "Do demonstracji algorytmów preprocessingu wykorzystano zestaw danych *sentiment140*, zawierający 1,6mln tweetów.\n",
    "<br>\n",
    "Dane zawierają następujące pola:\n",
    "- target: typ zdania (0 = negatywne, 2 = neutralne, 4 = pozytywne)\n",
    "- ids: ID tweeta (2087)\n",
    "- date: data tweeta (Sat May 16 23:58:44 UTC 2009)\n",
    "- flag: typ zapytania, jeśli brak to NO_QUERY.\n",
    "- user: nazwa użytkownika, który napisał tweet (robotickilldozr)\n",
    "- text: zawartość tweeta (Lyx is cool)\n",
    "\n",
    "<br>\n",
    "W dalszych etapie wykorzystamy pierwsze 10000 tweetów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", nrows=10000)\n",
    "data.columns = ['target','id','date','flag','user','text']\n",
    "text_data = data[[\"text\"]]\n",
    "text_data[\"text\"] = text_data[\"text\"].astype(str)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower casing \n",
    "Technika *lower casing* polega na formatowaniu tekstu do postaci małych liter. Jest to szczególnie istotne w zastosowaniach w których, ważnym jest, aby te same słowa zapisane w różny sposób były traktowane jako jeden wyraz.\n",
    "<br>\n",
    "Na przykład „WORD”, „Word” i „word” zostaną sformatowane do postaci „word”.\n",
    "<br>\n",
    "Użyteczność *lower casingu* ujawnia się w zastosowaniach powiązanych z częstością występowania wyrazów, poprzez zmniejszenie ilości duplikatów. Przykładem takiego zastosowania może być np. TFDIF (pol. ważenie częstością termów), która oblicza wagi słów w oparciu o liczbę ich wystąpień.\n",
    "<br>\n",
    "Należy jednak pamiętać, że wykorzystanie tej techniki nie jest pomocne we wszystkich zastosowaniach związanych z NLP, gdyż w niektórych wypadkach wielkość liter, może zawierać przydatne informacje o wyrażanych emocjach, czy po prostu częściach mowy/zdania.\n",
    "<br>\n",
    "Wszystkie najważniejsze biblioteki zawierają gotowe funkcje konwertujące tekst do postaci małych liter, bądź po prostu wykonują tą czynność same z siebie, jako element algorytmu. \n",
    "Przykładowe biblioteki i funkcje to:\n",
    "- NLTK\n",
    "- TensorFlow/Keras Tokenizer \n",
    "- sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "<br>\n",
    "W tym przypadku wykorzystano funkcje wbudowana w bibliotece *pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_lower_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>@kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                     text_lower_case  \n",
       "0  is upset that he can't update his facebook by ...  \n",
       "1  @kenichan i dived many times for the ball. man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @kwesidei not the whole crew   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[\"text_lower_case\"] = text_data[\"text\"].str.lower()\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usuwanie znaków interpunkcyjnych\n",
    "Technika ta polega na usuwaniu znaków interpunkcyjnych występujących w tekście.\n",
    "<br>\n",
    "Pozwala  na pozbycie się zbędnych szumów, które zazwyczaj nie niosą żadnych istotnych informacji.\n",
    "<br>\n",
    "Ponadto usunięcie znaków interpunkcyjnych pozwala traktować np. „Hey!” i „Hey” w ten sam sposób.\n",
    "<br>\n",
    "Python zawiera wbudowany string zawierający wszystkie znaki interpunkcyjne, pod poleceniem string.punctuation.\n",
    "<br>\n",
    "Wykorzystana funkcja została znaleziona pod linkiem:\n",
    "<br>\n",
    "https://www.pythondaddy.com/python/how-to-remove-punctuation-from-a-dataframe-in-pandas-and-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Kenichan I dived many times for the ball Manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                       text_no_punct  \n",
       "0  is upset that he cant update his Facebook by t...  \n",
       "1  Kenichan I dived many times for the ball Manag...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  nationwideclass no its not behaving at all im ...  \n",
       "4                       Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCTUATION = string.punctuation\n",
    "text_data.drop([\"text_lower_case\"], axis=1, inplace=True)\n",
    "def remove_punctuation(x: str) -> str:\n",
    "    try:\n",
    "        x = x.str.replace('[^\\w\\s]','')\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "text_data[\"text_no_punct\"] = text_data.apply(remove_punctuation)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usuwanie stop-słów \n",
    "Stop lista to lista zawierająca słowa nie wpływające na identyfikację dokumentu. Tego typu słowami są np. spójniki oraz słowa popularne. Istnieją gotowe stop listy dla różnych języków, w tym polskiego, która zawiera 350 słów.\n",
    "<br>\n",
    "Najważniejsze biblioteki posiadają wbudowane funkcje służące do filtracji słów ze stop list. Takimi bibliotekami są:\n",
    "- NLTK\n",
    "- TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punct</th>\n",
       "      <th>text_no_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "      <td>upset cant update Facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Kenichan I dived many times for the ball Manag...</td>\n",
       "      <td>Kenichan I dived many times ball Managed save ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>nationwideclass behaving im mad I cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>Kwesidei not the whole crew</td>\n",
       "      <td>Kwesidei whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                       text_no_punct  \\\n",
       "0  is upset that he cant update his Facebook by t...   \n",
       "1  Kenichan I dived many times for the ball Manag...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  nationwideclass no its not behaving at all im ...   \n",
       "4                       Kwesidei not the whole crew    \n",
       "\n",
       "                                        text_no_stop  \n",
       "0  upset cant update Facebook texting might cry r...  \n",
       "1  Kenichan I dived many times ball Managed save ...  \n",
       "2                   whole body feels itchy like fire  \n",
       "3         nationwideclass behaving im mad I cant see  \n",
       "4                                Kwesidei whole crew  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "'''\n",
    "remove_stopwords\n",
    "Funckja remove_stopwords wykorzystuje stop listę z biblioteki nltk.\n",
    "Pobiera tekst w postaci stringa, który następnie jest dzielony na pojedyncze wyrazy.\n",
    "Jeśli wyraz wyraz nie znajduje się na stop liście to jest on dodawany tuple (krotki).\n",
    "Funkcja na koniec łączy wszystkie wyrazy z krotki ze spacją jako separatorem.\n",
    "\n",
    "Parametry:\n",
    "    text: tekst z którego mają zostać usunięte stopwordy\n",
    "'''\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "text_data[\"text_no_stop\"] = text_data[\"text_no_punct\"].apply(lambda text: remove_stopwords(text))\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usuwanie częstych/rzadkich słów\n",
    "Zadaniem tego procesu, podobnie do usuwania stop-słów, jest odfiltrowanie zbędnych słów ze zbioru danych, które nie niosą żadnych istotnych dla zadania informacji.\n",
    "<br>\n",
    "Operację usuwania częstych słów można wykonać przy użyciu modułu *Counter* z biblioteki *collections*.\n",
    "<br>\n",
    "Należy zliczyć słowa występujące w dokumencie, a następnie wybrać np. 10 najczęstszych i je usunąć.\n",
    "<br>\n",
    "Taki sam proces można wykonać na najrzadszych słowach w korpusie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_stopfreq</th>\n",
       "      <th>text_no_stoprare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset update Facebook texting might cry result...</td>\n",
       "      <td>upset update Facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Kenichan dived many times ball Managed save 50...</td>\n",
       "      <td>Kenichan dived many times ball Managed save 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole body feels itchy fire</td>\n",
       "      <td>whole body feels itchy fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass behaving im mad see</td>\n",
       "      <td>nationwideclass behaving im mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>Kwesidei whole crew</td>\n",
       "      <td>Kwesidei whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                    text_no_stopfreq  \\\n",
       "0  upset update Facebook texting might cry result...   \n",
       "1  Kenichan dived many times ball Managed save 50...   \n",
       "2                        whole body feels itchy fire   \n",
       "3                nationwideclass behaving im mad see   \n",
       "4                                Kwesidei whole crew   \n",
       "\n",
       "                                    text_no_stoprare  \n",
       "0  upset update Facebook texting might cry result...  \n",
       "1  Kenichan dived many times ball Managed save 50...  \n",
       "2                        whole body feels itchy fire  \n",
       "3                nationwideclass behaving im mad see  \n",
       "4                                Kwesidei whole crew  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Zliczanie wszystkich słów w zbiorze\n",
    "counter = Counter()\n",
    "for text in text_data[\"text_no_stop\"].values:\n",
    "    for word in text.split():\n",
    "        counter[word] += 1\n",
    "counter.most_common(10)\n",
    "\n",
    "n_rare_words = 10\n",
    "# Tworzenie zbiorów 10 najczęstszych i najrzadszych wyrazów\n",
    "FREQWORDS = set([w for (w, wc) in counter.most_common(10)])\n",
    "RAREWORDS = set([w for (w, wc) in counter.most_common()[:-n_rare_words-1:-1]])\n",
    "\n",
    "'''\n",
    "remove_common_words\n",
    "Funkcja remove_common_words pobiera tekst w postaci stringa, \n",
    "który następnie jest dzielony na pojedyncze wyrazy.\n",
    "Jeśli wyraz wyraz nie znajduje się na liście częstych wyrazów to jest on dodawany tuple (krotki).\n",
    "Funkcja na koniec łączy wszystkie wyrazy z krotki ze spacją jako separatorem.\n",
    "\n",
    "Parametry:\n",
    "    text: tekst z którego mają zostać usunięte częste wyrazy\n",
    "'''\n",
    "def remove_common_words(text: str) -> str:\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "'''\n",
    "remove_uncommon_words\n",
    "Funkcja remove_uncommon_words pobiera tekst w postaci stringa, \n",
    "który następnie jest dzielony na pojedyncze wyrazy.\n",
    "Jeśli wyraz wyraz nie znajduje się na liście najrzadszych wyrazów to jest on dodawany tuple (krotki).\n",
    "Funkcja na koniec łączy wszystkie wyrazy z krotki ze spacją jako separatorem.\n",
    "\n",
    "Parametry:\n",
    "    text: tekst z którego mają zostać usunięte najrzadsze wyrazy\n",
    "'''\n",
    "def remove_uncommon_words(text: str) -> str:\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "text_data[\"text_no_stopfreq\"] = text_data[\"text_no_stop\"].apply(lambda text: remove_common_words(text))\n",
    "text_data[\"text_no_stoprare\"] = text_data[\"text_no_stopfreq\"].apply(lambda text: remove_uncommon_words(text))\n",
    "\n",
    "text_data.drop([\"text_no_punct\", \"text_no_stop\"], axis=1, inplace=True)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Technika stemmingu polega na usunięciu końcówki fleksyjnej słowa w celu pozostawienia jedynie tematu wyrazu (do rdzenia – ang. stem, stąd stemming).\n",
    "<br>\n",
    "Na przykład jeśli w korpusie znajdują się dwa słowa wywodzące się z jednego wyrazu np. „talk” i „talking” to algorytm stemmingu usunie końcówkę -ing i obydwa będą miały formę „talk”.\n",
    "<br>\n",
    "Jednym z najpopularniejszych algorytmów stemmingu jest algorytm Portera. Popularne biblioteki takie jak NLTK, scikit-learn czy TensorFlow nie zawierają algorytmów działających dla języka polskiego.\n",
    "Opis algorytmu Portera można znaleźć pod linkiem: \n",
    "<br>\n",
    "http://snowball.tartarus.org/algorithms/porter/stemmer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_stemmed_snow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't updat hi facebook by te...</td>\n",
       "      <td>is upset that he can't updat his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>@kenichan I dive mani time for the ball. manag...</td>\n",
       "      <td>@kenichan i dive mani time for the ball. manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole bodi feel itchi and like it on fire</td>\n",
       "      <td>my whole bodi feel itchi and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>@nationwideclass no, it' not behav at all. i'm...</td>\n",
       "      <td>@nationwideclass no, it not behav at all. i'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>@kwesidei not the whole crew</td>\n",
       "      <td>@kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  is upset that he can't updat hi facebook by te...   \n",
       "1  @kenichan I dive mani time for the ball. manag...   \n",
       "2       my whole bodi feel itchi and like it on fire   \n",
       "3  @nationwideclass no, it' not behav at all. i'm...   \n",
       "4                       @kwesidei not the whole crew   \n",
       "\n",
       "                                   text_stemmed_snow  \n",
       "0  is upset that he can't updat his facebook by t...  \n",
       "1  @kenichan i dive mani time for the ball. manag...  \n",
       "2       my whole bodi feel itchi and like it on fire  \n",
       "3  @nationwideclass no, it not behav at all. i'm ...  \n",
       "4                       @kwesidei not the whole crew  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Usuwanie niepotrzebnych kolumn\n",
    "text_data.drop([\"text_no_stopfreq\", \"text_no_stoprare\"], axis=1, inplace=True) \n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "'''\n",
    "stem_words\n",
    "Funkcja stem_words pobiera tekst w postaci stringa, który następnie jest dzielony na pojedyncze wyrazy.\n",
    "Każdy kolejny wyraz jest poddawany procesowi stemmmingu i zwracany do tuple (krotki).\n",
    "Funkcja na koniec łączy wszystkie wyrazy z krotki ze spacją jako separatorem.\n",
    "\n",
    "Parametry:\n",
    "    stemmer: obiekt zawierający wybrany algorytm stemmingu\n",
    "    text: tekst z którego mają zostać usunięte częste wyrazy\n",
    "'''\n",
    "def stem_words(stemmer: object, text: str) -> str:\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "text_data[\"text_stemmed\"] = text_data[\"text\"].apply(lambda text: stem_words(porter_stemmer,text))\n",
    "text_data[\"text_stemmed_snow\"] = text_data[\"text\"].apply(lambda text: stem_words(snowball_stemmer,text))\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematyzacja\n",
    "Technika o podobnym założeniu do stemmingu, jednak jej celem nie jest sprowadzenie słowa do postaci bazowej w sposób algorytmiczny, ale przez wykorzystanie gotowych słowników (słowo-sieci).\n",
    "<br>\n",
    "W ten sposób lematyzacja gwarantuje, że otrzymane słowo istnieje i jest poprawne gramatycznie (proces stemmingu, może uciąć końcówkę, bez której otrzymane wyraz nie jest prawdziwy słowem np. „wolves” → „wolv”).\n",
    "<br>\n",
    "Możliwym jest również, aby transformować słowa do różnych postaci np. rzeczownika, czasownika itd.\n",
    "Biblioteka *NLTK* zawiera WordNet dla m.in. języka angielskiego, jednak w przypadku języka polskiego, wymagane jest korzystanie zewnętrznego oprogramowania jak np. *Morfeusz2* lub zestawu narzędzi *LemmaPL*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't updat hi facebook by te...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>@kenichan I dive mani time for the ball. manag...</td>\n",
       "      <td>@Kenichan I dived many time for the ball. Mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole bodi feel itchi and like it on fire</td>\n",
       "      <td>my whole body feel itchy and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>@nationwideclass no, it' not behav at all. i'm...</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>@kwesidei not the whole crew</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  is upset that he can't updat hi facebook by te...   \n",
       "1  @kenichan I dive mani time for the ball. manag...   \n",
       "2       my whole bodi feel itchi and like it on fire   \n",
       "3  @nationwideclass no, it' not behav at all. i'm...   \n",
       "4                       @kwesidei not the whole crew   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many time for the ball. Mana...  \n",
       "2       my whole body feel itchy and like it on fire  \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                       @Kwesidei not the whole crew  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Usuwanie niepotrzebnej kolumny\n",
    "text_data.drop([\"text_stemmed_snow\"], axis=1, inplace=True) \n",
    "\n",
    "'''\n",
    "lemmatize_words\n",
    "Funkcja lemmatize_words pobiera tekst w postaci stringa, \n",
    "który następnie jest dzielony na pojedyncze wyrazy.\n",
    "Każdy kolejny wyraz jest poddawany procesowi lematyzacji i zwracany do tuple (krotki).\n",
    "Funkcja na koniec łączy wszystkie wyrazy z krotki ze spacją jako separatorem.\n",
    "\n",
    "Parametry:\n",
    "    lemmatizer: obiekt zawierający wybrany algorytm lematyzacji\n",
    "    text: tekst z którego mają zostać usunięte częste wyrazy\n",
    "'''\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(lemmatizer: object, text: str) -> str:\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "text_data[\"text_lemmatized\"] = text_data[\"text\"].apply(lambda text: lemmatize_words(lemmatizer, text))\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematyzacja bez określenia POS: cooking\n",
      "Lematyzacja przy określeniu POS: cook\n"
     ]
    }
   ],
   "source": [
    "example1 = lemmatizer.lemmatize(\"cooking\")\n",
    "example2 = lemmatizer.lemmatize(\"cooking\",\"v\")\n",
    "print(\"Lematyzacja bez określenia POS: \" + example1)\n",
    "print(\"Lematyzacja przy określeniu POS: \" + example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't updat hi facebook by te...</td>\n",
       "      <td>be upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>@kenichan I dive mani time for the ball. manag...</td>\n",
       "      <td>@Kenichan I dive many time for the ball. Manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole bodi feel itchi and like it on fire</td>\n",
       "      <td>my whole body feel itchy and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>@nationwideclass no, it' not behav at all. i'm...</td>\n",
       "      <td>@nationwideclass no, it's not behave at all. i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>@kwesidei not the whole crew</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  is upset that he can't updat hi facebook by te...   \n",
       "1  @kenichan I dive mani time for the ball. manag...   \n",
       "2       my whole bodi feel itchi and like it on fire   \n",
       "3  @nationwideclass no, it' not behav at all. i'm...   \n",
       "4                       @kwesidei not the whole crew   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  be upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dive many time for the ball. Manag...  \n",
       "2       my whole body feel itchy and like it on fire  \n",
       "3  @nationwideclass no, it's not behave at all. i...  \n",
       "4                       @Kwesidei not the whole crew  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Mapowanie części zdania\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "'''\n",
    "lemmatize_words\n",
    "Ulepszona funkcja lemmatize_words pobiera tekst w postaci stringa, \n",
    "który następnie jest dzielony na pojedyncze wyrazy.\n",
    "Tworzony jest obiekt zawierający pary (wyraz, część zdania), każdego słowa w zadanym tekście.\n",
    "Metoda lemmatize przyjmuje jako parametry wejściowe zadane słowo oraz typ części zdania.\n",
    "Funkcja na koniec łączy wszystkie wyrazy z krotki ze spacją jako separatorem.\n",
    "\n",
    "Parametry:\n",
    "    lemmatizer: obiekt zawierający wybrany algorytm lematyzacji\n",
    "    text: tekst z którego mają zostać usunięte częste wyrazy\n",
    "'''\n",
    "def lemmatize_words(lemmatizer: object, text: str) -> str:\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, \n",
    "                                                                         pos in pos_tagged_text])\n",
    "\n",
    "text_data[\"text_lemmatized\"] = text_data[\"text\"].apply(lambda text: lemmatize_words(lemmatizer, text))\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usuwanie emoji/emotikon\n",
    "Proces usuwania emotikon wykorzystuje gotową funkcję, którą można znaleźć pod linkiem:\n",
    "<br>\n",
    "https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "<br>\n",
    "Na tym etapie wykorzystwany jest język wyrażeń regularnych, który pozwala na ogólny opis parametrów wzorca zdania. Taki wzorzec może zostać wykorzystany do wyszukiwania zdań o takiej samej postaci.\n",
    "W tym przypadku tworzony jest wzorzec emoji/emotikon, który pozwala na szybkie wyszukanie wszystkich wystąpień w danym tekście i zastąpienie ich pustym polem.\n",
    "<br>\n",
    "Dokładniejszy opis języka wyrażeń regularnych można znaleźć pod linkiem:\n",
    "<br>\n",
    "https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like cats '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "remove_emoji\n",
    "Funckja remove_emoji wykorzystuje język wyrażeń regularnych w celu stworzenia szablonu wszystkich\n",
    "możliwych wystąpień emoji.\n",
    "Na podstawie szablonu funkcja zastępuje wszystkie wystąpienia emoji pustym polem.\n",
    "\n",
    "Parametry:\n",
    "    text: tekst z którego mają zostać usunięte wszystkie emoji\n",
    "'''\n",
    "def remove_emoji(text: str) -> str:\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emotikony\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbole i piktogramy\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # symbole transportu i map\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flagi (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"I like cats 🐱\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am very happy '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pobiernaie zbioru emotikon\n",
    "download(\"https://raw.githubusercontent.com/NeelShah18/emot/master/emot/emo_unicode.py\")\n",
    "from emo_unicode import EMOTICONS, UNICODE_EMO\n",
    "\n",
    "'''\n",
    "remove_emoticons\n",
    "Funckja remove_emoticons wykorzystuje język wyrażeń regularnych w celu stworzenia szablonu wszystkich\n",
    "możliwych wystąpień emotikon. Szablon jest tworzony na postawie gotowego zbioru emotikon.\n",
    "Na podstawie szablonu funkcja zastępuje wszystkie wystąpienia emotikon pustym polem.\n",
    "\n",
    "Parametry:\n",
    "    text: tekst z którego mają zostać usunięte wszystkie emotikon\n",
    "'''\n",
    "def remove_emoticons(text: str) -> str:\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoticons(\"I am very happy :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konwersja emotikon do słów\n",
    "W przypadku analizy tekstów z platform społecznościowych emoji oraz emotikony, mogą wnosić bardzo dużo przydatnych informacji np. o zabarwieniu zdania (czy jest negatywne, pozytywne, sarkastyczne itp.).\n",
    "<br>\n",
    "W takiej sytuacji usuwanie ich nie jest zalecane, jednak wymagane jest przetworzenie ich do postaci słownej, aby mogły zostać wykorzystane w procesie uczenia modelu/modelowania.\n",
    "<br>\n",
    "Istnieją gotowe listy słów korespondujących z emoji/emotikonami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sad Crying'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "convert_emoticons\n",
    "Funckja convert_emoticons wykorzystuje język wyrażeń regularnych w celu \n",
    "zastąpienia wszystkich wystąpień emotikon na opis słowny.\n",
    "\n",
    "Parametry:\n",
    "    text: tekst w którym wszystkie emotikony mają zostać zastąpione opisem słownym\n",
    "'''\n",
    "def convert_emoticons(text: str) -> str:\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"Hello ;-)\"\n",
    "convert_emoticons(text)\n",
    "\n",
    "text = \"I am sad :'(\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usuwanie URLów i tagów HTML\n",
    "Kolejnym krokiem, który może być konieczny w procesie preprocessingu, może być usuanie URLów i tagów HTML, w przypadku gdy pracujemy na danych które zostały np. zescrapowane ze strony internetowej.\n",
    "<br>\n",
    "Usuwanie urlów może zostać wykonane poprzez stworzenie np. prostego wzorca URLu.\n",
    "<br>\n",
    "Usuwanie tagów może zostać przeprowadzone ręcznie poprzez stworzenie wzorca tagów lub przy wykorzystaniu biblioteki BeautifulSoup4, która posiada gotową funkcję pozwalająca przetworzyć zescrapowany html do postaci czystego tekstu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Snippets of the album can be listened on distributor’s server under this link. '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "remove_urls\n",
    "Funckja remove_urls wykorzystuje język wyrażeń regularnych w celu stworzenia szablonu\n",
    "możliwych hiperlinków.\n",
    "Na podstawie szablonu wszystkie linki występujące w zadanym tekście są zastępowane pustym polem.\n",
    "\n",
    "Parametry:\n",
    "    text: tekst z którego mają zostać wykasowane wszystkie linki\n",
    "'''\n",
    "def remove_urls(text: str) -> str:\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "text = \"Snippets of the album can be listened on distributor’s server under this link. www.alternation.eu\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST\n",
      "ANOTHER TEST\n",
      " GOOGLE\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text: str) -> str:\n",
    "    \"\"\"Funckja wykorzystuje moduł BeautifulSoup do usunięcia wszystkich tagów HTML z zadanego tekstu\"\"\"\n",
    "    return BeautifulSoup(text, \"lxml\").text\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1>TEST</h1>\n",
    "<p>ANOTHER TEST</p>\n",
    "<a href=\"www.google.com\"> GOOGLE</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "print(remove_html(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poprawianie błędów językowych\n",
    "Technika ta wykorzystuje gotowe algorytmy do wykrywania błędów w zapisie, w celu ich korekty.\n",
    "<br>\n",
    "Wykrywanie i korekta błędów zapisu jest bardzo ważna w przypadku analizy tekstów z portali społecznościowych. Błędnie zapisane słowa mogą zawierać wiele przydatnych informacji, które nie zostaną poprawnie zainterpretowane.\n",
    "<br>\n",
    "Algorytm wykrywania błędów można znaleźć w bibliotece *pyspellchecker*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spelling mistake'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "'''\n",
    "correct_spellings\n",
    "Funckja correct_spellings wykorzystuje moduł SpellChecker do poprawiania błędów w zapisie słów.\n",
    "Pierwszym krokiem jest sprawdzenie poprawności wszystkich słów w zadanym tekście.\n",
    "Błędnie zapisane słowa są zapisywane w osobnej tablicy.\n",
    "Następnie wszystkie słowa, które wymagają poprawy są poprawiane i zapisywane w gotowej tablicy.\n",
    "Ostatnim krokiem jest połączenie tablicy do postaci stringa.\n",
    "\n",
    "Parametry:\n",
    "    text: tekst, który ma zostać zbadany pod kątem poprawności zapisu oraz w razie potrzeby poprawiony\n",
    "'''\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text: str) -> str:\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "text = \"speling mistaks\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza morfologiczna dla języka polskiego\n",
    "W przypadku języka polskiego występują jedynie nieliczne (open-source) narzędzia do NLP.\n",
    "<br>\n",
    "Jednym z dostępnych narzędzi jest darmowy Morfeusz2.\n",
    "<br>\n",
    "Link do projektu Morfeusz:\n",
    "<br>\n",
    "http://morfeusz.sgjp.pl/\n",
    "<br>\n",
    "Link do dokumentacji:\n",
    "<br>\n",
    "http://download.sgjp.pl/morfeusz/Morfeusz2.pdf\n",
    "<br>\n",
    "Opis działania Morfeusza (z dokumentacji znajdującej się na oficjalnej stronie projektu):\n",
    "<br>\n",
    "\"Dwie najważniejsze metody klasy Morfeusz to analyse oraz generate.\n",
    "Pierwsza z nich zwraca graf analizy morfoskładniowej dla podanego napisu\n",
    "w postaci listy trójek uporządkowanych reprezentujących pojedyncze interpretacje poszczególnych segmentów (czyli krawędzie w grafie analizy).\n",
    "<br>\n",
    "Każda trójka składa się z indeksów węzła początkowego i końcowego danej krawędzi\n",
    "oraz z interpretacji morfoskładniowej, stanowiącej etykietę krawędzi.\n",
    "<br>\n",
    "Interpretacja to piątka uporządkowana zawierająca:\n",
    "<br>\n",
    "- formę tekstową,\n",
    "- lemat (formę bazową/hasłową),\n",
    "- znacznik morfoskładniowy,\n",
    "- listę informacji o „pospolitości” rzeczownika (np. nazwa pospolita, marka, nazwisko),\n",
    "- listę kwalifikatorów stylistycznych (np. daw., pot., środ., wulg.) i dziedzinowych (np. bot., zool.). Segmenty nieznane słownikowi otrzymują specjalny znacznik ign oraz lemat równy formie tekstowej.\n",
    "\n",
    "<br>\n",
    "Metoda generate zwraca listę interpretacji morfoskładniowych (w postaci piątek, jw.) wszystkich form, dla których podany tekst stanowi formę bazową:\n",
    "<br>\n",
    "*morf.generate(u'piec')*\n",
    "<br>\n",
    "W odróżnieniu od analyse, generate akceptuje tylko napisy stanowiące pojedyncze słowo (bez spacji), w przeciwnym przypadku zostanie zgłoszony wyjątek.\"\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mam psa\n",
      "(0, 1, ('Mam', 'mama', 'subst:pl:gen:f', ['nazwa_pospolita'], []))\n",
      "(0, 1, ('Mam', 'mieć', 'fin:sg:pri:imperf', [], []))\n",
      "(0, 1, ('Mam', 'mamić', 'impt:sg:sec:imperf', [], []))\n",
      "(1, 2, ('psa', 'pies:s1', 'subst:sg:gen.acc:m2', ['nazwa_pospolita'], []))\n",
      "(1, 2, ('psa', 'pies:s2', 'subst:sg:gen.acc:m1', ['nazwa_pospolita'], ['pot.']))\n",
      "Lubisz grać w karty?\n",
      "(0, 1, ('Lubisz', 'lubić', 'fin:sg:sec:imperf', [], []))\n",
      "(1, 2, ('grać', 'grać', 'inf:imperf', [], []))\n",
      "(2, 3, ('w', 'w', 'prep:acc:nwok', [], []))\n",
      "(2, 3, ('w', 'w', 'prep:loc:nwok', [], []))\n",
      "(3, 4, ('karty', 'karty', 'subst:pl:nom.acc.voc:n:pt', ['nazwa_pospolita'], []))\n",
      "(3, 4, ('karty', 'kart', 'subst:pl:nom.acc.voc:m2', ['nazwa_pospolita'], ['pot.']))\n",
      "(3, 4, ('karty', 'kart', 'subst:pl:nom.acc.voc:m3', ['nazwa_pospolita'], []))\n",
      "(3, 4, ('karty', 'karta', 'subst:sg:gen:f', ['nazwa_pospolita'], []))\n",
      "(3, 4, ('karty', 'karta', 'subst:pl:nom.acc.voc:f', ['nazwa_pospolita'], []))\n",
      "(4, 5, ('?', '?', 'interp', [], []))\n",
      "asdasd123\n",
      "(0, 1, ('asdasd123', 'asdasd123', 'ign', [], []))\n"
     ]
    }
   ],
   "source": [
    "morf = morfeusz2.Morfeusz()\n",
    "\n",
    "for text in (u'Mam psa', u'Lubisz grać w karty?', u'asdasd123'):\n",
    "    print(text)\n",
    "    analysis = morf.analyse(text)\n",
    "    for interpretation in analysis:\n",
    "        print(interpretation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
